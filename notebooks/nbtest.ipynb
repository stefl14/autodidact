{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Union\n",
    "\n",
    "import click\n",
    "import torch\n",
    "import time\n",
    "import asyncio\n",
    "import layoutparser as lp\n",
    "import numpy as np\n",
    "from layoutparser.elements.layout_elements import TextBlock\n",
    "from pathlib import Path\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/stefan/.keys/solid-groove-215812-f6c753f1ed74.json'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest-asyncio in /home/stefan/.cache/pypoetry/virtualenvs/autodidact-n6tHuUMF-py3.8/lib/python3.8/site-packages (1.5.5)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nest-asyncio"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "async def perform_ocr(\n",
    "    ocr_agent: Union[lp.TesseractAgent, lp.GCVAgent],\n",
    "    image: np.array,\n",
    "    block: TextBlock,\n",
    "    left_pad: int = 15,\n",
    "    right_pad: int = 5,\n",
    "    top_pad: int = 5,\n",
    "    bottom_pad: int = 5,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Perform OCR on a block of text.\n",
    "\n",
    "    Args:\n",
    "        ocr_agent: The OCR agent to use.\n",
    "        image: The crop to perform OCR on.\n",
    "        block: The block to set the text of.\n",
    "        left_pad: The number of pixels to pad the left side of the block.\n",
    "        right_pad: The number of pixels to pad the right side of the block.\n",
    "        top_pad: The number of pixels to pad the top of the block.\n",
    "        bottom_pad: The number of pixels to pad the bottom of the block.\n",
    "    \"\"\"\n",
    "    # Pad to improve OCR accuracy as it's fairly tight.\n",
    "    segment_image = block.pad(\n",
    "        left=left_pad, right=right_pad, top=top_pad, bottom=bottom_pad\n",
    "    ).crop_image(image)\n",
    "\n",
    "    # Perform OCR and await the result.\n",
    "    text = await ocr_agent.detect(segment_image, return_only_text=True)\n",
    "\n",
    "    # Save OCR result\n",
    "    block.set(text=text, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def run_cli(\n",
    "    input_dir: Path,\n",
    "    output_dir: Path,\n",
    "    ocr_agent: str,\n",
    "    model: str,\n",
    "    detectron_threshold: float = 0.5,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Run cli to extract semi-structured JSON from document-AI + OCR.\n",
    "\n",
    "    Args:\n",
    "        input_dir: The directory containing the PDFs to parse.\n",
    "        output_dir: The directory to write the parsed PDFs to.\n",
    "        ocr_agent: The OCR agent to use.\n",
    "        model: The document AI model to use.\n",
    "        detectron_threshold: The threshold to use for Detectron2.\n",
    "    \"\"\"\n",
    "    input_dir = Path(input_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    model = lp.Detectron2LayoutModel(\n",
    "        config_path=f\"lp://PubLayNet/{model}\",  # In model catalog,\n",
    "        label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3: \"Table\", 4: \"Figure\"},\n",
    "        extra_config=[\n",
    "            \"MODEL.ROI_HEADS.SCORE_THRESH_TEST\",\n",
    "            detectron_threshold,\n",
    "        ],  # Optional\n",
    "    )\n",
    "\n",
    "    if ocr_agent == \"tesseract\":\n",
    "        ocr_agent = lp.TesseractAgent(languages=\"eng\")\n",
    "    elif ocr_agent == \"gcv\":\n",
    "        ocr_agent = lp.GCVAgent(languages=\"eng\")\n",
    "    input_dir = Path(input_dir)\n",
    "    for file in input_dir.iterdir():\n",
    "        file_name = file.name\n",
    "        if not file_name.endswith(\".pdf\"):\n",
    "            continue\n",
    "        _, pdf_images = lp.load_pdf(file, load_images=True)\n",
    "        block_pages = (\n",
    "            []\n",
    "        )  # list of pages of blocks (not captured by layoutparser, will put into a proper data\n",
    "        # structure later).\n",
    "        for ix, image in enumerate(pdf_images):\n",
    "            image_array = np.array(image)\n",
    "            detect_start = time.time()\n",
    "            layout = model.detect(image_array)  # perform computer vision\n",
    "            detect_end = time.time()\n",
    "            detect_time = detect_end - detect_start\n",
    "            # perform ocr on extracted blocks.\n",
    "            text_blocks = lp.Layout([b for b in layout if b.type == \"Text\"])\n",
    "            # convert to CustomTextBlock to add page_num attribute.\n",
    "            ocr_start = time.time()\n",
    "            for block in text_blocks:\n",
    "                perform_ocr(\n",
    "                    ocr_agent, image_array, block\n",
    "                )  # modify text blocks in-place\n",
    "            ocr_end = time.time()\n",
    "            ocr_time = ocr_end - ocr_start\n",
    "            block_pages.append([ix + 1] * len(text_blocks))\n",
    "\n",
    "        # flatten block_pages to a single list of blocks.\n",
    "        blocks = [block for page in block_pages for block in page]\n",
    "\n",
    "        # save extracted layout as json\n",
    "        text_block_dict = text_blocks.to_dict()\n",
    "        for ix, dic in enumerate(text_block_dict[\"blocks\"]):\n",
    "            dic[\"page_num\"] = blocks[ix]\n",
    "        file_name_without_ext = file_name.split(\".\")[0]\n",
    "        with open(output_dir / f\"{file_name_without_ext}.json\", \"w\") as f:\n",
    "            json.dump(text_block_dict, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mrun_cli\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m../downloads\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m../data/ocr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mocr_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgcv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmask_rcnn_X_101_32x8d_FPN_3x\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdetectron_threshold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.5\u001B[39;49m\n\u001B[1;32m      7\u001B[0m \u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [4]\u001B[0m, in \u001B[0;36mrun_cli\u001B[0;34m(input_dir, output_dir, ocr_agent, model, detectron_threshold)\u001B[0m\n\u001B[1;32m     18\u001B[0m input_dir \u001B[38;5;241m=\u001B[39m Path(input_dir)\n\u001B[1;32m     19\u001B[0m output_dir \u001B[38;5;241m=\u001B[39m Path(output_dir)\n\u001B[0;32m---> 20\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mlp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDetectron2LayoutModel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlp://PubLayNet/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mmodel\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# In model catalog,\u001B[39;49;00m\n\u001B[1;32m     22\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlabel_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mText\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mTitle\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mList\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mTable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mFigure\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m    \u001B[49m\u001B[43mextra_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mMODEL.ROI_HEADS.SCORE_THRESH_TEST\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     25\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdetectron_threshold\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[43m    \u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Optional\u001B[39;49;00m\n\u001B[1;32m     27\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ocr_agent \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtesseract\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m     30\u001B[0m     ocr_agent \u001B[38;5;241m=\u001B[39m lp\u001B[38;5;241m.\u001B[39mTesseractAgent(languages\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meng\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/autodidact-n6tHuUMF-py3.8/lib/python3.8/site-packages/layoutparser/models/detectron2/layoutmodel.py:119\u001B[0m, in \u001B[0;36mDetectron2LayoutModel.__init__\u001B[0;34m(self, config_path, model_path, label_map, extra_config, enforce_cpu, device)\u001B[0m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcfg \u001B[38;5;241m=\u001B[39m cfg\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel_map \u001B[38;5;241m=\u001B[39m label_map\n\u001B[0;32m--> 119\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/autodidact-n6tHuUMF-py3.8/lib/python3.8/site-packages/layoutparser/models/detectron2/layoutmodel.py:122\u001B[0m, in \u001B[0;36mDetectron2LayoutModel._create_model\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_create_model\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 122\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m \u001B[43mdetectron2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDefaultPredictor\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcfg\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/autodidact-n6tHuUMF-py3.8/lib/python3.8/site-packages/detectron2/engine/defaults.py:287\u001B[0m, in \u001B[0;36mDefaultPredictor.__init__\u001B[0;34m(self, cfg)\u001B[0m\n\u001B[1;32m    285\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, cfg):\n\u001B[1;32m    286\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcfg \u001B[38;5;241m=\u001B[39m cfg\u001B[38;5;241m.\u001B[39mclone()  \u001B[38;5;66;03m# cfg can be modified by model\u001B[39;00m\n\u001B[0;32m--> 287\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcfg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    288\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m    289\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(cfg\u001B[38;5;241m.\u001B[39mDATASETS\u001B[38;5;241m.\u001B[39mTEST):\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/autodidact-n6tHuUMF-py3.8/lib/python3.8/site-packages/detectron2/modeling/meta_arch/build.py:23\u001B[0m, in \u001B[0;36mbuild_model\u001B[0;34m(cfg)\u001B[0m\n\u001B[1;32m     21\u001B[0m meta_arch \u001B[38;5;241m=\u001B[39m cfg\u001B[38;5;241m.\u001B[39mMODEL\u001B[38;5;241m.\u001B[39mMETA_ARCHITECTURE\n\u001B[1;32m     22\u001B[0m model \u001B[38;5;241m=\u001B[39m META_ARCH_REGISTRY\u001B[38;5;241m.\u001B[39mget(meta_arch)(cfg)\n\u001B[0;32m---> 23\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMODEL\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m _log_api_usage(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodeling.meta_arch.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m meta_arch)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/autodidact-n6tHuUMF-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:907\u001B[0m, in \u001B[0;36mModule.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    903\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    904\u001B[0m                     non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[1;32m    905\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, non_blocking)\n\u001B[0;32m--> 907\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/autodidact-n6tHuUMF-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:578\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    576\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    577\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 578\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    580\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    581\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    582\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    583\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    588\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    589\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/autodidact-n6tHuUMF-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:578\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    576\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    577\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 578\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    580\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    581\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    582\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    583\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    588\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    589\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/autodidact-n6tHuUMF-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:601\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    597\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[1;32m    598\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[1;32m    599\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[1;32m    600\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 601\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    602\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[1;32m    603\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/autodidact-n6tHuUMF-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:905\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m    902\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[1;32m    903\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    904\u001B[0m                 non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[0;32m--> 905\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "run_cli(\n",
    "    input_dir=\"../downloads\",\n",
    "    output_dir=\"../data/ocr\",\n",
    "    ocr_agent=\"gcv\",\n",
    "    model=\"mask_rcnn_X_101_32x8d_FPN_3x\",\n",
    "    detectron_threshold=0.5\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Let's convert the above using asyncio."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "async def perform_ocr_async(ocr_agent, image, blocks):\n",
    "    tasks = []\n",
    "    for block in blocks:\n",
    "        tasks.append(asyncio.create_task(perform_ocr(ocr_agent, image, block)))\n",
    "    await asyncio.gather(*tasks)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "async def run_cli_async(\n",
    "    input_dir,\n",
    "    output_dir,\n",
    "    ocr_agent,\n",
    "    model,\n",
    "    detectron_threshold,\n",
    "):\n",
    "    input_dir = Path(input_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    model = lp.Detectron2LayoutModel(\n",
    "        config_path=f\"lp://PubLayNet/{model}\",  # In model catalog,\n",
    "        label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3: \"Table\", 4: \"Figure\"},\n",
    "        extra_config=[\n",
    "            \"MODEL.ROI_HEADS.SCORE_THRESH_TEST\",\n",
    "            detectron_threshold,\n",
    "        ],  # Optional\n",
    "    )\n",
    "    if ocr_agent == \"tesseract\":\n",
    "        ocr_agent = lp.TesseractAgent(languages=\"eng\")\n",
    "    elif ocr_agent == \"gcv\":\n",
    "        ocr_agent = lp.GCVAgent(languages=\"eng\")\n",
    "    for file in input_dir.iterdir():\n",
    "        file_name = file.name\n",
    "        if not file_name.endswith(\".pdf\"):\n",
    "            continue\n",
    "        _, pdf_images = lp.load_pdf(file, load_images=True)\n",
    "        block_pages = (\n",
    "            []\n",
    "        )  # list of pages of blocks (not captured by layoutparser, will put into a proper data\n",
    "        # structure later).\n",
    "        for ix, image in enumerate(pdf_images):\n",
    "            print(image)\n",
    "            image_array = np.array(image)\n",
    "            detect_start = time.time()\n",
    "            layout = model.detect(image_array)  # perform computer vision\n",
    "            # clear cuda cache\n",
    "            torch.cuda.empty_cache()\n",
    "            detect_end = time.time()\n",
    "            detect_time = detect_end - detect_start\n",
    "            # perform ocr on extracted blocks.\n",
    "            text_blocks = lp.Layout([b for b in layout if b.type == \"Text\"])\n",
    "            # convert to CustomTextBlock to add page_num attribute.\n",
    "            ocr_start = time.time()\n",
    "            await perform_ocr_async(ocr_agent, image_array, text_blocks)  # modify text blocks in-place\n",
    "            ocr_end = time.time()\n",
    "            ocr_time = ocr_end - ocr_start\n",
    "            print(ocr_time)\n",
    "            block_pages.append([ix + 1] * len(text_blocks))\n",
    "\n",
    "        # flatten block_pages to a single list of blocks.\n",
    "        blocks = [block for page in block_pages for block in page]\n",
    "\n",
    "        # save extracted layout as json\n",
    "        text_block_dict = text_blocks.to_dict()\n",
    "        for ix, dic in enumerate(text_block_dict[\"blocks\"]):\n",
    "            dic[\"page_num\"] = blocks[ix]\n",
    "        file_name_without_ext = file_name.split(\".\")[0]\n",
    "        with open(output_dir / f\"{file_name_without_ext}.json\", \"w\") as f:\n",
    "            json.dump(text_block_dict, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "async def main():\n",
    "    input_dir = \"../downloads\"\n",
    "    output_dir = \"../data/ocr\"\n",
    "    ocr_agent = \"gcv\"\n",
    "    model = \"mask_rcnn_X_101_32x8d_FPN_3x\"\n",
    "    detectron_threshold = 0.5\n",
    "    await run_cli_async(\n",
    "        input_dir,\n",
    "        output_dir,\n",
    "        ocr_agent,\n",
    "        model,\n",
    "        detectron_threshold,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001B[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.PpmImagePlugin.PpmImageFile image mode=RGB size=596x783 at 0x7F7E23260F10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/.cache/pypoetry/virtualenvs/autodidact-n6tHuUMF-py3.8/lib/python3.8/site-packages/detectron2/structures/image_list.py:99: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  max_size = (max_size + (stride - 1)) // stride * stride\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object str can't be used in 'await' expression",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [22]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43masyncio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/autodidact-n6tHuUMF-py3.8/lib/python3.8/site-packages/nest_asyncio.py:35\u001B[0m, in \u001B[0;36m_patch_asyncio.<locals>.run\u001B[0;34m(main, debug)\u001B[0m\n\u001B[1;32m     33\u001B[0m task \u001B[38;5;241m=\u001B[39m asyncio\u001B[38;5;241m.\u001B[39mensure_future(main)\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 35\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_until_complete\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m task\u001B[38;5;241m.\u001B[39mdone():\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/autodidact-n6tHuUMF-py3.8/lib/python3.8/site-packages/nest_asyncio.py:89\u001B[0m, in \u001B[0;36m_patch_loop.<locals>.run_until_complete\u001B[0;34m(self, future)\u001B[0m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m f\u001B[38;5;241m.\u001B[39mdone():\n\u001B[1;32m     87\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m     88\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEvent loop stopped before Future completed.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 89\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/asyncio/futures.py:175\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    173\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__log_traceback \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 175\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[1;32m    176\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\n",
      "File \u001B[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/asyncio/tasks.py:282\u001B[0m, in \u001B[0;36mTask.__step\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m    280\u001B[0m         result \u001B[38;5;241m=\u001B[39m coro\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    281\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 282\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[43mcoro\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mthrow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m    284\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_must_cancel:\n\u001B[1;32m    285\u001B[0m         \u001B[38;5;66;03m# Task is cancelled right before coro stops.\u001B[39;00m\n",
      "Input \u001B[0;32mIn [21]\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m      5\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmask_rcnn_X_101_32x8d_FPN_3x\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      6\u001B[0m detectron_threshold \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.5\u001B[39m\n\u001B[0;32m----> 7\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m run_cli_async(\n\u001B[1;32m      8\u001B[0m     input_dir,\n\u001B[1;32m      9\u001B[0m     output_dir,\n\u001B[1;32m     10\u001B[0m     ocr_agent,\n\u001B[1;32m     11\u001B[0m     model,\n\u001B[1;32m     12\u001B[0m     detectron_threshold,\n\u001B[1;32m     13\u001B[0m )\n",
      "Input \u001B[0;32mIn [18]\u001B[0m, in \u001B[0;36mrun_cli_async\u001B[0;34m(input_dir, output_dir, ocr_agent, model, detectron_threshold)\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# convert to CustomTextBlock to add page_num attribute.\u001B[39;00m\n\u001B[1;32m     43\u001B[0m ocr_start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m---> 44\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m perform_ocr_async(ocr_agent, image_array, text_blocks)  \u001B[38;5;66;03m# modify text blocks in-place\u001B[39;00m\n\u001B[1;32m     45\u001B[0m ocr_end \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m     46\u001B[0m ocr_time \u001B[38;5;241m=\u001B[39m ocr_end \u001B[38;5;241m-\u001B[39m ocr_start\n",
      "Input \u001B[0;32mIn [17]\u001B[0m, in \u001B[0;36mperform_ocr_async\u001B[0;34m(ocr_agent, image, blocks)\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m block \u001B[38;5;129;01min\u001B[39;00m blocks:\n\u001B[1;32m      4\u001B[0m     tasks\u001B[38;5;241m.\u001B[39mappend(asyncio\u001B[38;5;241m.\u001B[39mcreate_task(perform_ocr(ocr_agent, image, block)))\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mgather(\u001B[38;5;241m*\u001B[39mtasks)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/asyncio/tasks.py:349\u001B[0m, in \u001B[0;36mTask.__wakeup\u001B[0;34m(self, future)\u001B[0m\n\u001B[1;32m    347\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__wakeup\u001B[39m(\u001B[38;5;28mself\u001B[39m, future):\n\u001B[1;32m    348\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 349\u001B[0m         \u001B[43mfuture\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    350\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m    351\u001B[0m         \u001B[38;5;66;03m# This may also be a cancellation.\u001B[39;00m\n\u001B[1;32m    352\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__step(exc)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/asyncio/tasks.py:280\u001B[0m, in \u001B[0;36mTask.__step\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    277\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m exc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    278\u001B[0m         \u001B[38;5;66;03m# We use the `send` method directly, because coroutines\u001B[39;00m\n\u001B[1;32m    279\u001B[0m         \u001B[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001B[39;00m\n\u001B[0;32m--> 280\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[43mcoro\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    281\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    282\u001B[0m         result \u001B[38;5;241m=\u001B[39m coro\u001B[38;5;241m.\u001B[39mthrow(exc)\n",
      "Input \u001B[0;32mIn [16]\u001B[0m, in \u001B[0;36mperform_ocr\u001B[0;34m(ocr_agent, image, block, left_pad, right_pad, top_pad, bottom_pad)\u001B[0m\n\u001B[1;32m     23\u001B[0m segment_image \u001B[38;5;241m=\u001B[39m block\u001B[38;5;241m.\u001B[39mpad(\n\u001B[1;32m     24\u001B[0m     left\u001B[38;5;241m=\u001B[39mleft_pad, right\u001B[38;5;241m=\u001B[39mright_pad, top\u001B[38;5;241m=\u001B[39mtop_pad, bottom\u001B[38;5;241m=\u001B[39mbottom_pad\n\u001B[1;32m     25\u001B[0m )\u001B[38;5;241m.\u001B[39mcrop_image(image)\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# Perform OCR and await the result.\u001B[39;00m\n\u001B[0;32m---> 28\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m ocr_agent\u001B[38;5;241m.\u001B[39mdetect(segment_image, return_only_text\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# Save OCR result\u001B[39;00m\n\u001B[1;32m     31\u001B[0m block\u001B[38;5;241m.\u001B[39mset(text\u001B[38;5;241m=\u001B[39mtext, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mTypeError\u001B[0m: object str can't be used in 'await' expression"
     ]
    }
   ],
   "source": [
    "asyncio.run(main())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5368/1826504889.py:1: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  type(main())\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/plain": "coroutine"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(main())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}